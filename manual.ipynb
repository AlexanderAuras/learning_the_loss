{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values():\n",
    "    global x, y, w_0, b_0, w_1, b_1, alpha, a, b, c, d, f, g, z, h, j, k, l, n1, n2, n3, n4, m, o \n",
    "    print(\"======Forward=======\")\n",
    "    print(\"\\n-------Net----------\")\n",
    "    print(f\"\\tx: {x}\")\n",
    "    print(f\"\\tw_0: {w_0}\")\n",
    "    print(f\"\\ta = w_0*x: {a}\")\n",
    "    print(f\"\\tb_0: {b_0}\")\n",
    "    print(f\"\\tc = a+b_0: {c}\")\n",
    "    print(f\"\\td = max(0, c): {d}\")\n",
    "    print(f\"\\tw_1: {w_1}\")\n",
    "    print(f\"\\tf = w_1*d: {f}\")\n",
    "    print(f\"\\tb_1: {b_1}\")\n",
    "    print(f\"\\tg = g+b_1: {g}\")\n",
    "    print(\"\\n-------Softmax----------\")\n",
    "    print(f\"\\th = exp(g): {h}\")\n",
    "    print(f\"\\tj = sum(h): {j}\")\n",
    "    print(f\"\\tk = h/j: {k}\")\n",
    "    print(\"\\n-------NLL+Smoothing----------\")\n",
    "    print(f\"\\tl = -log(k): {l}\")\n",
    "    print(f\"\\ty: {torch.argmax(y)}\")\n",
    "    print(f\"\\talpha: {alpha}\")\n",
    "    print(f\"\\tn1 = (alpha ... alpha): {n1}\")\n",
    "    print(f\"\\tn2 = (alpha/(N-1) ... alpha/(N-1)): {n2}\")\n",
    "    print(f\"\\tn3 = 1.0-alpha: {n3}\")\n",
    "    print(f\"\\tn4 = (alpha/(N-1) ... 1.0-alpha  ... alpha/(N-1)): {n4}\")\n",
    "    print(f\"\\tm = l*n4: {m}\")\n",
    "    print(f\"\\to = sum(m): {o}\")\n",
    "\n",
    "def print_grads():\n",
    "    global x, y, w_0, b_0, w_1, b_1, alpha, a, b, c, d, f, g, z, h, j, k, l, n1, n2, n3, n4, m, o \n",
    "    print(\"======Backward======\")\n",
    "    print(\"\\n-------Net----------\")\n",
    "    print(f\"\\tw_0.grad = a.grad*x^T: {w_0.grad}\")\n",
    "    print(f\"\\ta.grad = c.grad*I: {a.grad}\")\n",
    "    print(f\"\\tb_0.grad = c.grad*I: {b_0.grad}\")\n",
    "    print(f\"\\tc.grad = d.grad*diag(\\{{c<0:1.0,0.0}}): {c.grad}\")\n",
    "    print(f\"\\td.grad = f.grad*w_1^T: {d.grad}\")\n",
    "    print(f\"\\tw_1.grad = f.grad*d^T: {w_1.grad}\")\n",
    "    print(f\"\\tf.grad = g.grad*I: {f.grad}\")\n",
    "    print(f\"\\tb_1.grad = g.grad*I: {b_1.grad}\")\n",
    "    print(f\"\\tg.grad = h.grad*diag(exp(g)): {g.grad}\")\n",
    "    print(\"\\n-------Softmax----------\")\n",
    "    print(f\"\\th.grad = k.grad*diag(j) + j.grad*(1.0 ... 1.0): {h.grad}\")\n",
    "    print(f\"\\tj.grad = k.grad*h^T: {j.grad}\")\n",
    "    print(f\"\\tk.grad = l.grad*diag(-1.0/k): {k.grad}\")\n",
    "    print(\"\\n-------NLL+Smoothing----------\")\n",
    "    print(f\"\\tl.grad = m.grad*diag(n4): {l.grad}\")\n",
    "    print(f\"\\talpha.grad = n1.grad*(1.0 ... 1.0)^T + n3.grad*(-1.0): {alpha.grad}\")\n",
    "    print(f\"\\tn1.grad = n2.grad*(1.0/(N-1) ... 1.0/(N-1)): {n1.grad}\")\n",
    "    print(f\"\\tn2.grad = n3.grad*I(with ij = 0.0): {n2.grad}\")\n",
    "    print(f\"\\tn3.grad = n4.grad*(0.0 ... 1.0 ... 0.0)^T: {n3.grad}\")\n",
    "    print(f\"\\tn4.grad = m.grad*diag(l): {n3.grad}\")\n",
    "    print(f\"\\tm.grad = (1.0 ... 1.0): {m.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 1.0])\n",
    "y = torch.tensor([0])\n",
    "\n",
    "w_0 = torch.randn((2,2), requires_grad=True)\n",
    "w_0.retain_grad()\n",
    "b_0 = torch.randn((2), requires_grad=True)\n",
    "b_0.retain_grad()\n",
    "w_1 = torch.randn((2,2), requires_grad=True)\n",
    "w_1.retain_grad()\n",
    "b_1 = torch.randn((2), requires_grad=True)\n",
    "b_1.retain_grad()\n",
    "alpha = torch.rand((1), requires_grad=True)\n",
    "alpha.retain_grad()\n",
    "\n",
    "a, b, c, d, f, g, z, h, j, k, l, n1, n2, n3, n4, m, o = None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward():\n",
    "    global x, y, w_0, b_0, w_1, b_1, alpha, a, b, c, d, f, g, z, h, j, k, l, n1, n2, n3, n4, m, o \n",
    "    a = w_0@x\n",
    "    a.retain_grad()\n",
    "    c = a+b_0\n",
    "    c.retain_grad()\n",
    "    d = torch.maximum(c, torch.zeros_like(c))\n",
    "    d.retain_grad()\n",
    "    f = w_1@d\n",
    "    f.retain_grad()\n",
    "    g = f+b_1\n",
    "    g.retain_grad()\n",
    "\n",
    "    z = g\n",
    "    z.retain_grad()\n",
    "\n",
    "    h = torch.exp(z)\n",
    "    h.retain_grad()\n",
    "    j = torch.sum(h)\n",
    "    j.retain_grad()\n",
    "    k = h/j\n",
    "    k.retain_grad()\n",
    "    l = -torch.log(k)\n",
    "    l.retain_grad()\n",
    "    n1 = torch.ones((2))*alpha\n",
    "    n1.retain_grad()\n",
    "    n2 = n1/(2-1)\n",
    "    n2.retain_grad()\n",
    "    n3 = 1.0-alpha\n",
    "    n3.retain_grad()\n",
    "    n4 = torch.scatter(n2, 0, y, n3)\n",
    "    n4.retain_grad()\n",
    "    m = l*n4\n",
    "    m.retain_grad()\n",
    "    o = torch.sum(m)\n",
    "    o.retain_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward():\n",
    "    global x, y, w_0, b_0, w_1, b_1, alpha, a, b, c, d, f, g, z, h, j, k, l, n1, n2, n3, n4, m, o \n",
    "    w_0.grad = None\n",
    "    b_0.grad = None\n",
    "    w_1.grad = None\n",
    "    b_1.grad = None\n",
    "    alpha.grad = None\n",
    "    o.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_net():\n",
    "    global x, y, w_0, b_0, w_1, b_1, alpha, a, b, c, d, f, g, z, h, j, k, l, n1, n2, n3, n4, m, o \n",
    "    with torch.no_grad():\n",
    "        w_0.copy_(w_0+0.01*w_0.grad)\n",
    "        b_0.copy_(b_0+0.01*b_0.grad)\n",
    "        w_1.copy_(w_1+0.01*w_1.grad)\n",
    "        b_1.copy_(b_1+0.01*b_1.grad)\n",
    "\n",
    "def update_alpha():\n",
    "    global x, y, w_0, b_0, w_1, b_1, alpha, a, b, c, d, f, g, z, h, j, k, l, n1, n2, n3, n4, m, o \n",
    "    with torch.no_grad():\n",
    "        alpha.copy_(alpha+0.01*alpha.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Forward=======\n",
      "\n",
      "-------Net----------\n",
      "\tx: tensor([1., 1.])\n",
      "\tw_0: tensor([[ 0.0321, -0.8767],\n",
      "        [ 0.1517,  0.5187]], requires_grad=True)\n",
      "\ta = w_0*x: tensor([-0.8447,  0.6704], grad_fn=<MvBackward>)\n",
      "\tb_0: tensor([-0.9749,  1.0810], requires_grad=True)\n",
      "\tc = a+b_0: tensor([-1.8196,  1.7514], grad_fn=<AddBackward0>)\n",
      "\td = max(0, c): tensor([0.0000, 1.7514], grad_fn=<MaximumBackward>)\n",
      "\tw_1: tensor([[-0.8707, -0.7331],\n",
      "        [ 0.1475,  0.2350]], requires_grad=True)\n",
      "\tf = w_1*d: tensor([-1.2839,  0.4115], grad_fn=<MvBackward>)\n",
      "\tb_1: tensor([0.4098, 1.0401], requires_grad=True)\n",
      "\tg = g+b_1: tensor([-0.8741,  1.4517], grad_fn=<AddBackward0>)\n",
      "\n",
      "-------Softmax----------\n",
      "\th = exp(g): tensor([0.4173, 4.2703], grad_fn=<ExpBackward>)\n",
      "\tj = sum(h): 4.687602519989014\n",
      "\tk = h/j: tensor([0.0890, 0.9110], grad_fn=<DivBackward0>)\n",
      "\n",
      "-------NLL+Smoothing----------\n",
      "\tl = -log(k): tensor([2.4190, 0.0932], grad_fn=<NegBackward>)\n",
      "\ty: 0\n",
      "\talpha: tensor([0.2914], requires_grad=True)\n",
      "\tn1 = (alpha ... alpha): tensor([0.2914, 0.2914], grad_fn=<MulBackward0>)\n",
      "\tn2 = (alpha/(N-1) ... alpha/(N-1)): tensor([0.2914, 0.2914], grad_fn=<DivBackward0>)\n",
      "\tn3 = 1.0-alpha: tensor([0.7086], grad_fn=<RsubBackward1>)\n",
      "\tn4 = (alpha/(N-1) ... 1.0-alpha  ... alpha/(N-1)): tensor([0.7086, 0.2914], grad_fn=<ScatterBackward0>)\n",
      "\tm = l*n3: tensor([1.7140, 0.0272], grad_fn=<MulBackward0>)\n",
      "\to = sum(m): 1.7411956787109375\n",
      "\n",
      "\n",
      "======Backward======\n",
      "\n",
      "-------Net----------\n",
      "\tw_0.grad = a.grad*x^T: tensor([[0.0000, 0.0000],\n",
      "        [0.5998, 0.5998]])\n",
      "\ta.grad = c.grad*I: tensor([0.0000, 0.5998])\n",
      "\tb_0.grad = c.grad*I: tensor([0.0000, 0.5998])\n",
      "\tc.grad = d.grad*diag(\\{c<0:1.0,0.0}): tensor([0.0000, 0.5998])\n",
      "\td.grad = f.grad*w_1^T: tensor([0.6308, 0.5998])\n",
      "\tw_1.grad = f.grad*d^T: tensor([[-0.0000, -1.0851],\n",
      "        [ 0.0000,  1.0851]])\n",
      "\tf.grad = g.grad*I: tensor([-0.6196,  0.6196])\n",
      "\tb_1.grad = g.grad*I: tensor([-0.6196,  0.6196])\n",
      "\tg.grad = h.grad*diag(exp(g)): tensor([-0.6196,  0.6196])\n",
      "\n",
      "-------Softmax----------\n",
      "\th.grad = k.grad*diag(j) + j.grad*(1.0 ... 1.0): tensor([-1.4849,  0.1451])\n",
      "\tj.grad = k.grad*h^T: 0.21332865953445435\n",
      "\tk.grad = l.grad*diag(-1.0/k): tensor([-7.9604, -0.3199])\n",
      "\n",
      "-------NLL+Smoothing----------\n",
      "\tl.grad = m.grad*diag(n4): tensor([0.7086, 0.2914])\n",
      "\talpha.grad = n1.grad*(1.0 ... 1.0)^T + n3.grad*(-1.0): tensor([-2.3258])\n",
      "\tn1.grad = n2.grad*(1.0/(N-1) ... 1.0/(N-1)): tensor([0.0000, 0.0932])\n",
      "\tn2.grad = n3.grad*I(with ij = 0.0): tensor([0.0000, 0.0932])\n",
      "\tn3.grad = n4.grad*(0.0 ... 1.0 ... 0.0)^T: tensor([2.4190])\n",
      "\tn4.grad = m.grad*diag(l): tensor([2.4190])\n",
      "\tm.grad = (1.0 ... 1.0): tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "forward()\n",
    "print_values()\n",
    "backward()\n",
    "print(\"\\n\")\n",
    "print_grads()\n",
    "update_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Forward=======\n",
      "\n",
      "-------Net----------\n",
      "\tx: tensor([1., 1.])\n",
      "\tw_0: tensor([[ 0.0321, -0.8767],\n",
      "        [ 0.1577,  0.5247]], requires_grad=True)\n",
      "\ta = w_0*x: tensor([-0.8447,  0.6824], grad_fn=<MvBackward>)\n",
      "\tb_0: tensor([-0.9749,  1.0870], requires_grad=True)\n",
      "\tc = a+b_0: tensor([-1.8196,  1.7694], grad_fn=<AddBackward0>)\n",
      "\td = max(0, c): tensor([0.0000, 1.7694], grad_fn=<MaximumBackward>)\n",
      "\tw_1: tensor([[-0.8707, -0.7439],\n",
      "        [ 0.1475,  0.2458]], requires_grad=True)\n",
      "\tf = w_1*d: tensor([-1.3163,  0.4350], grad_fn=<MvBackward>)\n",
      "\tb_1: tensor([0.4036, 1.0463], requires_grad=True)\n",
      "\tg = g+b_1: tensor([-0.9126,  1.4813], grad_fn=<AddBackward0>)\n",
      "\n",
      "-------Softmax----------\n",
      "\th = exp(g): tensor([0.4015, 4.3987], grad_fn=<ExpBackward>)\n",
      "\tj = sum(h): 4.800204753875732\n",
      "\tk = h/j: tensor([0.0836, 0.9164], grad_fn=<DivBackward0>)\n",
      "\n",
      "-------NLL+Smoothing----------\n",
      "\tl = -log(k): tensor([2.4813, 0.0873], grad_fn=<NegBackward>)\n",
      "\ty: 0\n",
      "\talpha: tensor([0.2914], requires_grad=True)\n",
      "\tn1 = (alpha ... alpha): tensor([0.2914, 0.2914], grad_fn=<MulBackward0>)\n",
      "\tn2 = (alpha/(N-1) ... alpha/(N-1)): tensor([0.2914, 0.2914], grad_fn=<DivBackward0>)\n",
      "\tn3 = 1.0-alpha: tensor([0.7086], grad_fn=<RsubBackward1>)\n",
      "\tn4 = (alpha/(N-1) ... 1.0-alpha  ... alpha/(N-1)): tensor([0.7086, 0.2914], grad_fn=<ScatterBackward0>)\n",
      "\tm = l*n3: tensor([1.7582, 0.0255], grad_fn=<MulBackward0>)\n",
      "\to = sum(m): 1.7836402654647827\n",
      "\n",
      "\n",
      "======Backward======\n",
      "\n",
      "-------Net----------\n",
      "\tw_0.grad = a.grad*x^T: tensor([[0.0000, 0.0000],\n",
      "        [0.6185, 0.6185]])\n",
      "\ta.grad = c.grad*I: tensor([0.0000, 0.6185])\n",
      "\tb_0.grad = c.grad*I: tensor([0.0000, 0.6185])\n",
      "\tc.grad = d.grad*diag(\\{c<0:1.0,0.0}): tensor([0.0000, 0.6185])\n",
      "\td.grad = f.grad*w_1^T: tensor([0.6363, 0.6185])\n",
      "\tw_1.grad = f.grad*d^T: tensor([[-0.0000, -1.1058],\n",
      "        [ 0.0000,  1.1058]])\n",
      "\tf.grad = g.grad*I: tensor([-0.6249,  0.6249])\n",
      "\tb_1.grad = g.grad*I: tensor([-0.6249,  0.6249])\n",
      "\tg.grad = h.grad*diag(exp(g)): tensor([-0.6249,  0.6249])\n",
      "\n",
      "-------Softmax----------\n",
      "\th.grad = k.grad*diag(j) + j.grad*(1.0 ... 1.0): tensor([-1.5567,  0.1421])\n",
      "\tj.grad = k.grad*h^T: 0.20832444727420807\n",
      "\tk.grad = l.grad*diag(-1.0/k): tensor([-8.4723, -0.3180])\n",
      "\n",
      "-------NLL+Smoothing----------\n",
      "\tl.grad = m.grad*diag(n4): tensor([0.7086, 0.2914])\n",
      "\talpha.grad = n1.grad*(1.0 ... 1.0)^T + n3.grad*(-1.0): tensor([-2.3940])\n",
      "\tn1.grad = n2.grad*(1.0/(N-1) ... 1.0/(N-1)): tensor([0.0000, 0.0873])\n",
      "\tn2.grad = n3.grad*I(with ij = 0.0): tensor([0.0000, 0.0873])\n",
      "\tn3.grad = n4.grad*(0.0 ... 1.0 ... 0.0)^T: tensor([2.4813])\n",
      "\tn4.grad = m.grad*diag(l): tensor([2.4813])\n",
      "\tm.grad = (1.0 ... 1.0): tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "forward()\n",
    "print_values()\n",
    "backward()\n",
    "print(\"\\n\")\n",
    "print_grads()\n",
    "update_alpha()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2782eec6307456fcfffdbec4f62579a0f367b071ed339ece342e4b52c5497c05"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('MA': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
